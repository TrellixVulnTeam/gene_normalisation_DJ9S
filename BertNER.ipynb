{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert NER on SageMaker using PyTorch\n",
    "\n",
    "This uses the Biocreative II gene mention dataset https://biocreative.bioinformatics.udel.edu/tasks/biocreative-ii/task-1a-gene-mention-tagging/\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import logging\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "logging.basicConfig(level=\"INFO\", handlers=[logging.StreamHandler(sys.stdout)],\n",
    "                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket and role set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-04 19:51:52,617 - sagemaker.analytics - WARNING - pandas failed to import. Analytics features will be impaired or broken.\n",
      "2020-10-04 19:51:53,090 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2020-10-04 19:51:53,256 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "sm_session = sagemaker.session.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# role=get_execution_role()\n",
    "role =\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20181222T162635\".format(account_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = \"aegovan-data\"\n",
    "\n",
    "data_bucket_prefix = \"Biocreative-gene-mention\"\n",
    "\n",
    "s3_uri_data = \"s3://{}/{}/train data\".format(data_bucket, data_bucket_prefix)\n",
    "s3_uri_train = \"{}/{}\".format(s3_uri_data, \"train.in\")\n",
    "s3_uri_classes = \"{}/{}\".format(s3_uri_data, \"GENE.eval\")\n",
    "\n",
    "s3_uri_test = \"s3://{}/{}/test data\".format(data_bucket, data_bucket_prefix , \"test.in\")\n",
    "\n",
    "s3_output_path = \"s3://{}/{}/output\".format(data_bucket, data_bucket_prefix)\n",
    "s3_code_path = \"s3://{}/{}/code\".format(data_bucket, data_bucket_prefix)\n",
    "s3_checkpoint = \"s3://{}/{}/checkpoint\".format(data_bucket, data_bucket_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "This shows you how to train BERT on SageMaker using SPOT instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_full =  {\n",
    "    \"train\" : s3_uri_train,\n",
    "    \"class\" : s3_uri_classes\n",
    "}\n",
    "\n",
    "inputs = inputs_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_localcheckpoint_dir=\"/opt/ml/checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p3.2xlarge\"\n",
    "instance_type_gpu_map = {\"ml.p3.8xlarge\":4, \"ml.p3.2xlarge\": 1, \"ml.p3.16xlarge\":8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\n",
    "\"epochs\" : 30,\n",
    "\"earlystoppingpatience\" : 5,\n",
    "# Increasing batch size might end up with CUDA OOM error, increase grad accumulation instead\n",
    "\"batch\" : 8 * instance_type_gpu_map[instance_type],\n",
    "\"trainfile\" :s3_uri_train.split(\"/\")[-1],\n",
    "\"classfile\":s3_uri_classes.split(\"/\")[-1],\n",
    "# The number of steps to accumulate gradients for\n",
    "\"gradaccumulation\" : 4,\n",
    "\"log-level\":\"INFO\",\n",
    "# This param depends on your model max pos embedding size or when large you might end up with CUDA OOM error    \n",
    "\"maxseqlen\" : 512,\n",
    "# Make sure the lr is quite small, as this is a pretrained model..\n",
    "\"lr\":0.00001,\n",
    "# Use finetuning (set to 1), if you only want to change the weights in the final classification layer.. \n",
    "\"finetune\": 0,\n",
    "\"checkpointdir\" : sm_localcheckpoint_dir,\n",
    "# Checkpoints once every n epochs\n",
    "\"checkpointfreq\": 2,\n",
    "\"log-level\" : \"INFO\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 30,\n",
       " 'earlystoppingpatience': 5,\n",
       " 'batch': 8,\n",
       " 'trainfile': 'train.in',\n",
       " 'classfile': 'GENE.eval',\n",
       " 'gradaccumulation': 4,\n",
       " 'log-level': 'INFO',\n",
       " 'maxseqlen': 512,\n",
       " 'lr': 1e-05,\n",
       " 'finetune': 0,\n",
       " 'checkpointdir': '/opt/ml/checkpoints/',\n",
       " 'checkpointfreq': 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://aegovan-data/Biocreative-gene-mention/train data/train.in',\n",
       " 'class': 's3://aegovan-data/Biocreative-gene-mention/train data/GENE.eval'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{\"Name\": \"TrainLoss\",\n",
    "                     \"Regex\": \"###score: train_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"ValidationLoss\",\n",
    "                     \"Regex\": \"###score: val_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"TrainScore\",\n",
    "                     \"Regex\": \"###score: train_score### (\\d*[.]?\\d*)\"}\n",
    "                   ,{\"Name\": \"ValidationScore\",\n",
    "                     \"Regex\": \"###score: val_score### (\\d*[.]?\\d*)\"}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True if you need spot instance\n",
    "use_spot = True\n",
    "train_max_run_secs =   2*24 * 60 * 60\n",
    "spot_wait_sec =  5 * 60\n",
    "max_wait_time_secs = train_max_run_secs +  spot_wait_sec\n",
    "\n",
    "if not use_spot:\n",
    "    max_wait_time_secs = None\n",
    "    \n",
    "# During local mode, no spot.., use smaller dataset\n",
    "if instance_type == 'local':\n",
    "    use_spot = False\n",
    "    max_wait_time_secs = 0\n",
    "    wait = True\n",
    "    # Use smaller dataset to run locally\n",
    "    inputs = inputs_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type = \"bc2-ner-bert\"\n",
    "base_name = \"{}\".format(job_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-04 20:16:02,884 - sagemaker - INFO - Creating training-job with name: bc2-ner-bert-2020-10-04-09-15-57-317\n",
      "2020-10-04 09:16:06 Starting - Starting the training job...\n",
      "2020-10-04 09:16:08 Starting - Launching requested ML instances...\n",
      "2020-10-04 09:17:06 Starting - Preparing the instances for training......\n",
      "2020-10-04 09:18:09 Downloading - Downloading input data...\n",
      "2020-10-04 09:18:43 Training - Downloading the training image.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-10-04 09:19:59,616 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-10-04 09:19:59,641 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:02,677 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:02,989 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:02,989 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:02,990 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:02,990 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmplf_lc8ai/module_dir\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.4.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.4.0)\u001b[0m\n",
      "\u001b[34mCollecting transformers==3.0.1\n",
      "  Downloading transformers-3.0.1-py3-none-any.whl (757 kB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==0.23.1\n",
      "  Downloading scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting seqeval==0.0.17\n",
      "  Downloading seqeval-0.0.17.tar.gz (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.8.0-rc4\n",
      "  Downloading tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (1.16.4)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\n",
      "2020-10-04 09:19:58 Training - Training image download completed. Training in progress.\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.9.27-cp36-cp36m-manylinux2010_x86_64.whl (662 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (4.42.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (20.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.23.1->-r requirements.txt (line 3)) (1.2.2)\u001b[0m\n",
      "\u001b[34mCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.23.1->-r requirements.txt (line 3)) (0.15.1)\u001b[0m\n",
      "\u001b[34mCollecting Keras>=2.2.4\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==3.0.1->-r requirements.txt (line 2)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==3.0.1->-r requirements.txt (line 2)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==3.0.1->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==3.0.1->-r requirements.txt (line 2)) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==3.0.1->-r requirements.txt (line 2)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==3.0.1->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==3.0.1->-r requirements.txt (line 2)) (1.25.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval==0.0.17->-r requirements.txt (line 4)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval==0.0.17->-r requirements.txt (line 4)) (5.3.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: seqeval, default-user-module-name, sacremoses\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.17-py3-none-any.whl size=7638 sha256=ff7208046e439d0704424b32cb868dd1f310fa7520f6543a95d9e0d292738c7d\n",
      "  Stored in directory: /root/.cache/pip/wheels/a0/8f/33/b734af42c4a11d13fa4412f5019cde239430c25ca654f29536\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=27888 sha256=277a7065c0c202c9f8a064330aedd72fa6d0741df494beb41b744991e9864331\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-07dkjd0h/wheels/69/39/f6/0e952481fb0a7b9b1057387dd7b9fcc025f402368f35da6eea\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=2116810774985e453bff11db1076b8664718f89a2804634675237df939f803e0\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\u001b[0m\n",
      "\u001b[34mSuccessfully built seqeval default-user-module-name sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, regex, sacremoses, filelock, sentencepiece, transformers, threadpoolctl, scikit-learn, Keras, seqeval, default-user-module-name\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.21.2\n",
      "    Uninstalling scikit-learn-0.21.2:\n",
      "      Successfully uninstalled scikit-learn-0.21.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed Keras-2.4.3 default-user-module-name-1.0.0 filelock-3.0.12 regex-2020.9.27 sacremoses-0.0.43 scikit-learn-0.23.1 sentencepiece-0.1.91 seqeval-0.0.17 threadpoolctl-2.1.0 tokenizers-0.8.0rc4 transformers-3.0.1\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:14,406 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"class\": \"/opt/ml/input/data/class\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"classfile\": \"GENE.eval\",\n",
      "        \"checkpointfreq\": 2,\n",
      "        \"lr\": 1e-05,\n",
      "        \"batch\": 8,\n",
      "        \"trainfile\": \"train.in\",\n",
      "        \"gradaccumulation\": 4,\n",
      "        \"finetune\": 0,\n",
      "        \"log-level\": \"INFO\",\n",
      "        \"maxseqlen\": 512,\n",
      "        \"epochs\": 30,\n",
      "        \"earlystoppingpatience\": 5,\n",
      "        \"checkpointdir\": \"/opt/ml/checkpoints/\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"class\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"bc2-ner-bert-2020-10-04-09-15-57-317\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://aegovan-data/Biocreative-gene-mention/code/bc2-ner-bert-2020-10-04-09-15-57-317/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"main\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"main.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch\":8,\"checkpointdir\":\"/opt/ml/checkpoints/\",\"checkpointfreq\":2,\"classfile\":\"GENE.eval\",\"earlystoppingpatience\":5,\"epochs\":30,\"finetune\":0,\"gradaccumulation\":4,\"log-level\":\"INFO\",\"lr\":1e-05,\"maxseqlen\":512,\"trainfile\":\"train.in\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=main.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"class\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"class\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=main\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://aegovan-data/Biocreative-gene-mention/code/bc2-ner-bert-2020-10-04-09-15-57-317/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"class\":\"/opt/ml/input/data/class\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch\":8,\"checkpointdir\":\"/opt/ml/checkpoints/\",\"checkpointfreq\":2,\"classfile\":\"GENE.eval\",\"earlystoppingpatience\":5,\"epochs\":30,\"finetune\":0,\"gradaccumulation\":4,\"log-level\":\"INFO\",\"lr\":1e-05,\"maxseqlen\":512,\"trainfile\":\"train.in\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"class\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"bc2-ner-bert-2020-10-04-09-15-57-317\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://aegovan-data/Biocreative-gene-mention/code/bc2-ner-bert-2020-10-04-09-15-57-317/source/sourcedir.tar.gz\",\"module_name\":\"main\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"main.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch\",\"8\",\"--checkpointdir\",\"/opt/ml/checkpoints/\",\"--checkpointfreq\",\"2\",\"--classfile\",\"GENE.eval\",\"--earlystoppingpatience\",\"5\",\"--epochs\",\"30\",\"--finetune\",\"0\",\"--gradaccumulation\",\"4\",\"--log-level\",\"INFO\",\"--lr\",\"1e-05\",\"--maxseqlen\",\"512\",\"--trainfile\",\"train.in\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CLASS=/opt/ml/input/data/class\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_CLASSFILE=GENE.eval\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINTFREQ=2\u001b[0m\n",
      "\u001b[34mSM_HP_LR=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH=8\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINFILE=train.in\u001b[0m\n",
      "\u001b[34mSM_HP_GRADACCUMULATION=4\u001b[0m\n",
      "\u001b[34mSM_HP_FINETUNE=0\u001b[0m\n",
      "\u001b[34mSM_HP_LOG-LEVEL=INFO\u001b[0m\n",
      "\u001b[34mSM_HP_MAXSEQLEN=512\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=30\u001b[0m\n",
      "\u001b[34mSM_HP_EARLYSTOPPINGPATIENCE=5\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINTDIR=/opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python main.py --batch 8 --checkpointdir /opt/ml/checkpoints/ --checkpointfreq 2 --classfile GENE.eval --earlystoppingpatience 5 --epochs 30 --finetune 0 --gradaccumulation 4 --log-level INFO --lr 1e-05 --maxseqlen 512 --trainfile train.in\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m{'trainfile': 'train.in', 'traindir': '/opt/ml/input/data/train', 'classfile': 'GENE.eval', 'classdir': '/opt/ml/input/data/class', 'outdir': '/opt/ml/output/data', 'modeldir': '/opt/ml/model', 'checkpointdir': '/opt/ml/checkpoints/', 'checkpointfreq': '2', 'earlystoppingpatience': 5, 'epochs': 30, 'gradaccumulation': 4, 'batch': 8, 'lr': 1e-05, 'finetune': 0, 'maxseqlen': 512, 'log_level': 'INFO'}\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:17,483 - filelock - INFO - Lock 140574043905888 acquired on /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:17,484 - transformers.file_utils - INFO - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpct7sohrp\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:17,612 - transformers.file_utils - INFO - storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt in cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:17,612 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:17,613 - filelock - INFO - Lock 140574043905888 released on /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:17,613 - transformers.tokenization_utils_base - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:17,780 - transformers.tokenization_utils_base - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-10-04 09:20:36,949 - transformers.configuration_utils - INFO - loading configuration file /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:36,949 - transformers.configuration_utils - INFO - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:36,950 - transformers.modeling_utils - INFO - loading weights file /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:40,693 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\u001b[0m\n",
      "\u001b[34m2020-10-04 09:20:40,693 - transformers.modeling_utils - INFO - All the weights of BertForTokenClassification were initialized from the model checkpoint at /opt/ml/checkpoints/.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:22:20,823 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-04 09:22:21,364 - trainer - INFO - Train set result details: 0.2125057683433318\u001b[0m\n",
      "\u001b[34m2020-10-04 09:22:21,364 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:24:27,012 - trainer - INFO - Validation set result details: 0.355314511632622 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:24:27,012 - trainer - INFO - Snapshotting because the current score 0.355314511632622 is greater than None \u001b[0m\n",
      "\u001b[34m2020-10-04 09:24:27,012 - trainer - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2020-10-04 09:24:27,013 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:24:27,413 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:24:27,413 - trainer - INFO - Checkpoint model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2020-10-04 09:24:27,414 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:24:27,931 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:24:27,932 - trainer - INFO - Run    227     0       375     9/375         2% 0.057105 0.025953       0.2125       0.3553\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.05710452596346537\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.025953208461093405\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.2125057683433318\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.355314511632622\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:26:04,888 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-04 09:26:05,600 - trainer - INFO - Train set result details: 0.3524729960204662\u001b[0m\n",
      "\u001b[34m2020-10-04 09:26:05,600 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-10-04 09:28:11,756 - trainer - INFO - Validation set result details: 0.43967669377363616 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:28:11,812 - trainer - INFO - Snapshotting because the current score 0.43967669377363616 is greater than 0.355314511632622 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:28:11,812 - trainer - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2020-10-04 09:28:11,813 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:28:12,271 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:28:12,272 - trainer - INFO - Run    451     1       750     9/375         2% 0.043748 0.023825       0.3525       0.4397\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.043748344724377\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.02382524897996336\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.3524729960204662\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.43967669377363616\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:29:49,801 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-04 09:29:50,521 - trainer - INFO - Train set result details: 0.44802825191289\u001b[0m\n",
      "\u001b[34m2020-10-04 09:29:50,521 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:31:57,092 - trainer - INFO - Validation set result details: 0.46241955482080654 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:31:57,151 - trainer - INFO - Snapshotting because the current score 0.46241955482080654 is greater than 0.43967669377363616 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:31:57,151 - trainer - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2020-10-04 09:31:57,152 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:31:57,710 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:31:57,711 - trainer - INFO - Run    677     2      1125     9/375         2% 0.037540 0.018836       0.4480       0.4624\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.037540296190728746\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.01883551162931447\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.44802825191289\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.46241955482080654\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:33:35,339 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-04 09:33:36,086 - trainer - INFO - Train set result details: 0.4695630829651448\u001b[0m\n",
      "\u001b[34m2020-10-04 09:33:36,086 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:35:42,635 - trainer - INFO - Validation set result details: 0.5058007488797496 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:35:42,691 - trainer - INFO - Snapshotting because the current score 0.5058007488797496 is greater than 0.46241955482080654 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:35:42,692 - trainer - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2020-10-04 09:35:42,692 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:35:43,210 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:35:43,210 - trainer - INFO - Checkpoint model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2020-10-04 09:35:43,212 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:35:43,751 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:35:43,752 - trainer - INFO - Run    903     3      1500     9/375         2% 0.033746 0.017140       0.4696       0.5058\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.03374637069304784\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.017140324169032585\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.4695630829651448\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.5058007488797496\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-10-04 09:37:21,464 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-04 09:37:22,201 - trainer - INFO - Train set result details: 0.5287243203346045\u001b[0m\n",
      "\u001b[34m2020-10-04 09:37:22,201 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:39:28,706 - trainer - INFO - Validation set result details: 0.5591105689253631 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:39:28,764 - trainer - INFO - Snapshotting because the current score 0.5591105689253631 is greater than 0.5058007488797496 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:39:28,765 - trainer - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2020-10-04 09:39:28,765 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:39:29,284 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:39:29,285 - trainer - INFO - Run   1128     4      1875     9/375         2% 0.028725 0.018503       0.5287       0.5591\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.02872493493370712\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.01850323225026174\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.5287243203346045\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.5591105689253631\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:41:07,016 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-04 09:41:07,553 - trainer - INFO - Train set result details: 0.5697030699547055\u001b[0m\n",
      "\u001b[34m2020-10-04 09:41:07,553 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:43:14,076 - trainer - INFO - Validation set result details: 0.5866368769058906 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:43:14,132 - trainer - INFO - Snapshotting because the current score 0.5866368769058906 is greater than 0.5591105689253631 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:43:14,132 - trainer - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2020-10-04 09:43:14,133 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:43:14,657 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:43:14,657 - trainer - INFO - Run   1353     5      2250     9/375         2% 0.026109 0.016876       0.5697       0.5866\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.026108736872052153\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.016875810482015367\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.5697030699547055\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.5866368769058906\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:44:52,421 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-04 09:44:52,951 - trainer - INFO - Train set result details: 0.5973630831643001\u001b[0m\n",
      "\u001b[34m2020-10-04 09:44:52,951 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-10-04 09:46:59,465 - trainer - INFO - Validation set result details: 0.5875483911912165 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:46:59,521 - trainer - INFO - Snapshotting because the current score 0.5875483911912165 is greater than 0.5866368769058906 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:46:59,522 - trainer - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2020-10-04 09:46:59,522 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:46:59,983 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:46:59,984 - trainer - INFO - Checkpoint model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2020-10-04 09:46:59,985 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:47:00,475 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:47:00,476 - trainer - INFO - Run   1579     6      2625     9/375         2% 0.024555 0.016300       0.5974       0.5875\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.024555448256898673\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.01629999018611852\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.5973630831643001\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.5875483911912165\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:48:38,123 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-04 09:48:38,864 - trainer - INFO - Train set result details: 0.6203958907541969\u001b[0m\n",
      "\u001b[34m2020-10-04 09:48:38,865 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:50:45,424 - trainer - INFO - Validation set result details: 0.6089980173859998 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:50:45,480 - trainer - INFO - Snapshotting because the current score 0.6089980173859998 is greater than 0.5875483911912165 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:50:45,480 - trainer - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2020-10-04 09:50:45,481 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:50:46,003 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:50:46,003 - trainer - INFO - Run   1805     7      3000     9/375         2% 0.022422 0.015979       0.6204       0.6090\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.02242229758699735\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.015978718890459275\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.6203958907541969\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6089980173859998\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:52:23,524 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-04 09:52:24,252 - trainer - INFO - Train set result details: 0.6509824198552224\u001b[0m\n",
      "\u001b[34m2020-10-04 09:52:24,253 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:54:30,739 - trainer - INFO - Validation set result details: 0.6165630350545193 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:54:30,795 - trainer - INFO - Snapshotting because the current score 0.6165630350545193 is greater than 0.6089980173859998 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:54:30,796 - trainer - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2020-10-04 09:54:30,796 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:54:31,361 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:54:31,361 - trainer - INFO - Run   2030     8      3375     9/375         2% 0.020192 0.015846       0.6510       0.6166\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.0201917005777359\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.015846403356835556\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.6509824198552224\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6165630350545193\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-10-04 09:56:08,980 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-04 09:56:09,702 - trainer - INFO - Train set result details: 0.6797318205260444\u001b[0m\n",
      "\u001b[34m2020-10-04 09:56:09,702 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:58:16,326 - trainer - INFO - Validation set result details: 0.6160117495429067 \u001b[0m\n",
      "\u001b[34m2020-10-04 09:58:16,326 - trainer - INFO - Checkpoint model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2020-10-04 09:58:16,327 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2020-10-04 09:58:16,890 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2020-10-04 09:58:16,891 - trainer - INFO - Run   2256     9      3750     9/375         2% 0.018518 0.016352       0.6797       0.6160\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.018517839019497235\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.01635212992269468\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.6797318205260444\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6160117495429067\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 09:59:54,434 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-04 09:59:55,224 - trainer - INFO - Train set result details: 0.6844697951775992\u001b[0m\n",
      "\u001b[34m2020-10-04 09:59:55,224 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 10:02:01,937 - trainer - INFO - Validation set result details: 0.6136925874042503 \u001b[0m\n",
      "\u001b[34m2020-10-04 10:02:01,937 - trainer - INFO - Run   2481    10      4125     9/375         2% 0.017728 0.015641       0.6845       0.6137\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.017728199587514003\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.015641475731196505\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.6844697951775992\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6136925874042503\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-04 10:03:39,530 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-04 10:03:40,081 - trainer - INFO - Train set result details: 0.6922777417261519\u001b[0m\n",
      "\u001b[34m2020-10-04 10:03:40,081 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-10-04 10:04:18 Stopping - Stopping the training job\u001b[34m2020-10-04 10:05:46,989 - trainer - INFO - Validation set result details: 0.6027381500230096 \u001b[0m\n",
      "\u001b[34m2020-10-04 10:05:46,990 - trainer - INFO - Run   2706    11      4500     9/375         2% 0.016705 0.016400       0.6923       0.6027\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.016705091308491925\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.01639985454439496\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.6922777417261519\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6027381500230096\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\n",
      "2020-10-04 10:06:34 Uploading - Uploading generated training model\n",
      "2020-10-04 10:07:37 Stopped - Training job stopped\n",
      "Training seconds: 2832\n",
      "Billable seconds: 850\n",
      "Managed Spot Training savings: 70.0%\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='main.py',\n",
    "                    source_dir = 'src',\n",
    "                    role=role,\n",
    "                    framework_version =\"1.4.0\",\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type=instance_type,\n",
    "                    hyperparameters = hp,\n",
    "                    output_path=s3_output_path,\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    volume_size=30,\n",
    "                    code_location=s3_code_path,\n",
    "                    debugger_hook_config=False,\n",
    "                    base_job_name =base_name,  \n",
    "                    use_spot_instances = use_spot,\n",
    "                    max_run =  train_max_run_secs,\n",
    "                    max_wait = max_wait_time_secs,   \n",
    "                    checkpoint_s3_uri=s3_checkpoint,\n",
    "                    checkpoint_local_path=sm_localcheckpoint_dir\n",
    "                    )\n",
    "\n",
    "estimator.fit(inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference container\n",
    "Ideally the server containing should already have all the required dependencies installed to reduce start up time and ensure that the runtime enviornment is consistent. This can be implemented using a custom docker image.\n",
    "\n",
    "But for this demo, to simplify, we will let the Pytorch container script model install the dependencies during start up. As a result, you will see some of the initial ping requests fail, until all dependencies are installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-10-04 10:07:37 Starting - Preparing the instances for training\n",
      "2020-10-04 10:07:37 Downloading - Downloading input data\n",
      "2020-10-04 10:07:37 Training - Training image download completed. Training in progress.\n",
      "2020-10-04 10:07:37 Stopping - Stopping the training job\n",
      "2020-10-04 10:07:37 Uploading - Uploading generated training model\n",
      "2020-10-04 10:07:37 Stopped - Training job stopped\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "training_job = \"bc2-ner-bert-2020-10-04-09-15-57-317\"\n",
    "estimator = sagemaker.estimator.Estimator.attach(training_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-05 00:35:41,222 - sagemaker - INFO - Creating model with name: pytorch-inference-2020-10-04-13-35-41-222\n",
      "2020-10-05 00:35:45,969 - sagemaker - INFO - Creating endpoint with name pytorch-inference-2020-10-04-13-35-43-250\n",
      "--------"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "role = role\n",
    "\n",
    "model_uri = estimator.model_data\n",
    "\n",
    "model = PyTorchModel(model_data=model_uri,\n",
    "                     role=role,\n",
    "                     framework_version='1.4.0',\n",
    "                     py_version = \"py3\",\n",
    "                     entry_point='serve.py',\n",
    "                     source_dir='src'\n",
    "                    \n",
    "                    )\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    \n",
    "    def serialize(self, x):\n",
    "        return x\n",
    "    \n",
    "    def deserialize(self,x, content_type):\n",
    "        payload_bytes = json.loads( x.read().decode(\"utf-8\") )\n",
    "        return payload_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = Predictor()\n",
    "predictor.deserializer = Predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Hailey-Hailey disease is caused by mutations in  ATP2C1  encoding a novel Ca(2+) pump.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "I-GENE ##ity\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "B-GENE %\n",
      "I-GENE ##virus\n",
      "B-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "B-GENE Thus\n",
      "B-GENE as\n",
      "B-GENE -\n",
      "I-GENE 2\n",
      "I-GENE ,\n",
      "I-GENE Sur\n",
      "B-GENE )\n",
      "I-GENE ,\n",
      "I-GENE Y\n",
      "I-GENE ,\n",
      "I-GENE Sur\n",
      "I-GENE ##f\n",
      "B-GENE ,\n",
      "I-GENE and\n",
      "I-GENE Y\n",
      "I-GENE ##15\n",
      "I-GENE ##17\n",
      "I-GENE ##2\n",
      "B-GENE Sur\n",
      "I-GENE ##f\n",
      "I-GENE -\n",
      "I-GENE 5\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "I-GENE )\n",
      "I-GENE and\n",
      "I-GENE 2\n",
      "I-GENE (\n",
      "I-GENE /\n",
      "I-GENE th\n",
      "I-GENE p\n",
      "I-GENE [PAD]\n",
      "B-GENE [PAD]\n",
      "I-GENE [PAD]\n",
      "B-GENE Zhu\n",
      "B-GENE [PAD]\n",
      "I-GENE ##F\n",
      "I-GENE ##H\n",
      "I-GENE ##F\n",
      "B-GENE -\n",
      "I-GENE 3\n",
      "I-GENE ,\n",
      "I-GENE C\n",
      "I-GENE /\n",
      "B-GENE ##B\n",
      "I-GENE ##P\n",
      "I-GENE ,\n",
      "I-GENE and\n",
      "I-GENE C\n",
      "B-GENE E\n",
      "I-GENE ##B\n",
      "I-GENE ##P\n",
      "I-GENE beta\n",
      "I-GENE ,\n",
      "I-GENE [PAD]\n",
      "I-GENE [PAD]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from datasets.biocreative_dataset import BiocreativeDataset\n",
    "d = BiocreativeDataset(\"tmp/train.in\", None)\n",
    "for i in range( 0,len(d),5):\n",
    "\n",
    "    data = [d[j][0][0] for j in range(i, i+5)]\n",
    "    #print(i,len(d), data)\n",
    "    data_bytes=(\"\\n\".join(data)).encode(\"utf-8\")\n",
    "    response_bytes  = predictor.predict(data_bytes,  \n",
    "                                    initial_args={ \"Accept\":\"text/json\", \"ContentType\" : \"text/csv\" }\n",
    "                                   )\n",
    "   \n",
    "    for r in response_bytes:\n",
    "        for i in r:\n",
    "            if i[\"entity\"] != \"O\":\n",
    "                print(i[\"entity\"], i[\"raw_token\"])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-05 00:29:40,004 - sagemaker - INFO - Deleting endpoint configuration with name: pytorch-inference-2020-10-04-10-16-55-684\n",
      "2020-10-05 00:29:42,109 - sagemaker - INFO - Deleting endpoint with name: pytorch-inference-2020-10-04-10-16-55-684\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
