{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert NER on SageMaker using PyTorch\n",
    "\n",
    "This uses the Biocreative II gene mention dataset https://biocreative.bioinformatics.udel.edu/tasks/biocreative-ii/task-1a-gene-mention-tagging/\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import logging\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "logging.basicConfig(level=\"INFO\", handlers=[logging.StreamHandler(sys.stdout)],\n",
    "                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket and role set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-01 21:58:52,374 - sagemaker.analytics - WARNING - pandas failed to import. Analytics features will be impaired or broken.\n",
      "2020-10-01 21:58:52,496 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "sm_session = sagemaker.session.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# role=get_execution_role()\n",
    "role =\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20181222T162635\".format(account_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = \"aegovan-data\"\n",
    "\n",
    "data_bucket_prefix = \"Biocreative-gene-mention\"\n",
    "\n",
    "s3_uri_data = \"s3://{}/{}/train data\".format(data_bucket, data_bucket_prefix)\n",
    "s3_uri_train = \"{}/{}\".format(s3_uri_data, \"train.in\")\n",
    "s3_uri_classes = \"{}/{}\".format(s3_uri_data, \"GENE.eval\")\n",
    "\n",
    "s3_uri_test = \"s3://{}/{}/test data\".format(data_bucket, data_bucket_prefix , \"test.in\")\n",
    "\n",
    "s3_output_path = \"s3://{}/{}/output\".format(data_bucket, data_bucket_prefix)\n",
    "s3_code_path = \"s3://{}/{}/code\".format(data_bucket, data_bucket_prefix)\n",
    "s3_checkpoint = \"s3://{}/{}/checkpoint\".format(data_bucket, data_bucket_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "This shows you how to train BERT on SageMaker using SPOT instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_full =  {\n",
    "    \"train\" : s3_uri_train,\n",
    "    \"class\" : s3_uri_classes\n",
    "}\n",
    "\n",
    "inputs = inputs_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_localcheckpoint_dir=\"/opt/ml/checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p3.8xlarge\"\n",
    "instance_type_gpu_map = {\"ml.p3.8xlarge\":4, \"ml.p3.2xlarge\": 1, \"ml.p3.16xlarge\":8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\n",
    "\"epochs\" : 10,\n",
    "\"earlystoppingpatience\" : 3,\n",
    "# Increasing batch size might end up with CUDA OOM error, increase grad accumulation instead\n",
    "\"batch\" : 8 * instance_type_gpu_map[instance_type],\n",
    "\"trainfile\" :s3_uri_train.split(\"/\")[-1],\n",
    "\"classfile\":s3_uri_classes.split(\"/\")[-1],\n",
    "# The number of steps to accumulate gradients for\n",
    "\"gradaccumulation\" : 4,\n",
    "\"log-level\":\"INFO\",\n",
    "# This param depends on your model max pos embedding size or when large you might end up with CUDA OOM error    \n",
    "\"maxseqlen\" : 512,\n",
    "# Make sure the lr is quite small, as this is a pretrained model..\n",
    "\"lr\":0.00001,\n",
    "# Use finetuning (set to 1), if you only want to change the weights in the final classification layer.. \n",
    "\"finetune\": 0,\n",
    "\"checkpointdir\" : sm_localcheckpoint_dir,\n",
    "# Checkpoints once every n epochs\n",
    "\"checkpointfreq\": 2\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 10,\n",
       " 'earlystoppingpatience': 3,\n",
       " 'batch': 32,\n",
       " 'trainfile': 'train.in',\n",
       " 'classfile': 'GENE.eval',\n",
       " 'gradaccumulation': 4,\n",
       " 'log-level': 'INFO',\n",
       " 'maxseqlen': 512,\n",
       " 'lr': 1e-05,\n",
       " 'finetune': 0,\n",
       " 'checkpointdir': '/opt/ml/checkpoints/',\n",
       " 'checkpointfreq': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://aegovan-data/Biocreative-gene-mention/train data/train.in',\n",
       " 'class': 's3://aegovan-data/Biocreative-gene-mention/train data/GENE.eval'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{\"Name\": \"TrainLoss\",\n",
    "                     \"Regex\": \"###score: train_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"ValidationLoss\",\n",
    "                     \"Regex\": \"###score: val_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"TrainScore\",\n",
    "                     \"Regex\": \"###score: train_score### (\\d*[.]?\\d*)\"}\n",
    "                   ,{\"Name\": \"ValidationScore\",\n",
    "                     \"Regex\": \"###score: val_score### (\\d*[.]?\\d*)\"}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True if you need spot instance\n",
    "use_spot = True\n",
    "train_max_run_secs =   2*24 * 60 * 60\n",
    "spot_wait_sec =  5 * 60\n",
    "max_wait_time_secs = train_max_run_secs +  spot_wait_sec\n",
    "\n",
    "if not use_spot:\n",
    "    max_wait_time_secs = None\n",
    "    \n",
    "# During local mode, no spot.., use smaller dataset\n",
    "if instance_type == 'local':\n",
    "    use_spot = False\n",
    "    max_wait_time_secs = 0\n",
    "    wait = True\n",
    "    # Use smaller dataset to run locally\n",
    "    inputs = inputs_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type = \"bc2-ner-bert\"\n",
    "base_name = \"{}\".format(job_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-02 11:42:54,141 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2020-10-02 11:42:57,041 - sagemaker - INFO - Creating training-job with name: bc2-ner-bert-2020-10-02-01-42-54-263\n",
      "2020-10-02 01:42:58 Starting - Starting the training job...\n",
      "2020-10-02 01:43:00 Starting - Launching requested ML instances...\n",
      "2020-10-02 01:43:56 Starting - Preparing the instances for training.........\n",
      "2020-10-02 01:45:31 Downloading - Downloading input data...\n",
      "2020-10-02 01:46:03 Training - Downloading the training image...\n",
      "2020-10-02 01:46:57 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-10-02 01:46:58,211 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-10-02 01:46:58,253 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-10-02 01:46:59,699 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-10-02 01:46:59,983 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-10-02 01:46:59,983 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-10-02 01:46:59,983 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-10-02 01:46:59,983 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpmnwffi83/module_dir\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.4.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.4.0)\u001b[0m\n",
      "\u001b[34mCollecting transformers==3.0.1\n",
      "  Downloading transformers-3.0.1-py3-none-any.whl (757 kB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==0.23.1\n",
      "  Downloading scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting seqeval==0.0.17\n",
      "  Downloading seqeval-0.0.17.tar.gz (20 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (20.4)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.8.0-rc4\n",
      "  Downloading tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (4.42.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.9.27-cp36-cp36m-manylinux2010_x86_64.whl (662 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (0.7)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (1.16.4)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.23.1->-r requirements.txt (line 3)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.23.1->-r requirements.txt (line 3)) (0.15.1)\u001b[0m\n",
      "\u001b[34mCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting Keras>=2.2.4\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==3.0.1->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==3.0.1->-r requirements.txt (line 2)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==3.0.1->-r requirements.txt (line 2)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==3.0.1->-r requirements.txt (line 2)) (1.25.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==3.0.1->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==3.0.1->-r requirements.txt (line 2)) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==3.0.1->-r requirements.txt (line 2)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval==0.0.17->-r requirements.txt (line 4)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval==0.0.17->-r requirements.txt (line 4)) (5.3.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: seqeval, default-user-module-name, sacremoses\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.17-py3-none-any.whl size=7638 sha256=9e28f8051ba2ce4a67b258ef78d1c03390a2dcce147c8bb73693b9c93724446d\n",
      "  Stored in directory: /root/.cache/pip/wheels/a0/8f/33/b734af42c4a11d13fa4412f5019cde239430c25ca654f29536\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=21300 sha256=2bf2a99d4e63cf5cefe0fe306cf07f662a9746943c74678824e46cc6e354a410\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-einseppp/wheels/4b/e8/fe/e78052a4b55b1b90db185469617582911c1379e3594e8ddc1e\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=ae1f7e77ee3a84c95d81f4838572c913c121094985dcd02969821c7b6c19c9fc\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\u001b[0m\n",
      "\u001b[34mSuccessfully built seqeval default-user-module-name sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, regex, sacremoses, filelock, sentencepiece, transformers, threadpoolctl, scikit-learn, Keras, seqeval, default-user-module-name\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.21.2\n",
      "    Uninstalling scikit-learn-0.21.2:\n",
      "      Successfully uninstalled scikit-learn-0.21.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed Keras-2.4.3 default-user-module-name-1.0.0 filelock-3.0.12 regex-2020.9.27 sacremoses-0.0.43 scikit-learn-0.23.1 sentencepiece-0.1.91 seqeval-0.0.17 threadpoolctl-2.1.0 tokenizers-0.8.0rc4 transformers-3.0.1\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:11,325 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"class\": \"/opt/ml/input/data/class\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"classfile\": \"GENE.eval\",\n",
      "        \"checkpointfreq\": 2,\n",
      "        \"lr\": 1e-05,\n",
      "        \"batch\": 32,\n",
      "        \"trainfile\": \"train.in\",\n",
      "        \"gradaccumulation\": 4,\n",
      "        \"finetune\": 0,\n",
      "        \"log-level\": \"INFO\",\n",
      "        \"maxseqlen\": 512,\n",
      "        \"epochs\": 10,\n",
      "        \"earlystoppingpatience\": 3,\n",
      "        \"checkpointdir\": \"/opt/ml/checkpoints/\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"class\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"bc2-ner-bert-2020-10-02-01-42-54-263\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://aegovan-data/Biocreative-gene-mention/code/bc2-ner-bert-2020-10-02-01-42-54-263/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"main\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"main.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch\":32,\"checkpointdir\":\"/opt/ml/checkpoints/\",\"checkpointfreq\":2,\"classfile\":\"GENE.eval\",\"earlystoppingpatience\":3,\"epochs\":10,\"finetune\":0,\"gradaccumulation\":4,\"log-level\":\"INFO\",\"lr\":1e-05,\"maxseqlen\":512,\"trainfile\":\"train.in\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=main.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"class\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"class\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=main\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://aegovan-data/Biocreative-gene-mention/code/bc2-ner-bert-2020-10-02-01-42-54-263/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"class\":\"/opt/ml/input/data/class\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch\":32,\"checkpointdir\":\"/opt/ml/checkpoints/\",\"checkpointfreq\":2,\"classfile\":\"GENE.eval\",\"earlystoppingpatience\":3,\"epochs\":10,\"finetune\":0,\"gradaccumulation\":4,\"log-level\":\"INFO\",\"lr\":1e-05,\"maxseqlen\":512,\"trainfile\":\"train.in\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"class\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"bc2-ner-bert-2020-10-02-01-42-54-263\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://aegovan-data/Biocreative-gene-mention/code/bc2-ner-bert-2020-10-02-01-42-54-263/source/sourcedir.tar.gz\",\"module_name\":\"main\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"main.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch\",\"32\",\"--checkpointdir\",\"/opt/ml/checkpoints/\",\"--checkpointfreq\",\"2\",\"--classfile\",\"GENE.eval\",\"--earlystoppingpatience\",\"3\",\"--epochs\",\"10\",\"--finetune\",\"0\",\"--gradaccumulation\",\"4\",\"--log-level\",\"INFO\",\"--lr\",\"1e-05\",\"--maxseqlen\",\"512\",\"--trainfile\",\"train.in\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CLASS=/opt/ml/input/data/class\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_CLASSFILE=GENE.eval\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINTFREQ=2\u001b[0m\n",
      "\u001b[34mSM_HP_LR=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH=32\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINFILE=train.in\u001b[0m\n",
      "\u001b[34mSM_HP_GRADACCUMULATION=4\u001b[0m\n",
      "\u001b[34mSM_HP_FINETUNE=0\u001b[0m\n",
      "\u001b[34mSM_HP_LOG-LEVEL=INFO\u001b[0m\n",
      "\u001b[34mSM_HP_MAXSEQLEN=512\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_EARLYSTOPPINGPATIENCE=3\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINTDIR=/opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python main.py --batch 32 --checkpointdir /opt/ml/checkpoints/ --checkpointfreq 2 --classfile GENE.eval --earlystoppingpatience 3 --epochs 10 --finetune 0 --gradaccumulation 4 --log-level INFO --lr 1e-05 --maxseqlen 512 --trainfile train.in\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m{'trainfile': 'train.in', 'traindir': '/opt/ml/input/data/train', 'classfile': 'GENE.eval', 'classdir': '/opt/ml/input/data/class', 'outdir': '/opt/ml/output/data', 'modeldir': '/opt/ml/model', 'checkpointdir': '/opt/ml/checkpoints/', 'checkpointfreq': '2', 'earlystoppingpatience': 3, 'epochs': 10, 'gradaccumulation': 4, 'batch': 32, 'lr': 1e-05, 'finetune': 0, 'maxseqlen': 512, 'log_level': 'INFO'}\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:14,896 - filelock - INFO - Lock 140140593188032 acquired on /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:14,896 - transformers.file_utils - INFO - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp8ex1sx83\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:15,002 - transformers.file_utils - INFO - storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt in cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:15,002 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:15,003 - filelock - INFO - Lock 140140593188032 released on /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:15,003 - transformers.tokenization_utils_base - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:15,150 - transformers.tokenization_utils_base - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:33,641 - trainer - INFO - Loading checkpoint /opt/ml/checkpoints/checkpoint.pt , found 1 checkpoint files\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:38,191 - filelock - INFO - Lock 140138766067192 acquired on /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391.lock\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:38,191 - transformers.file_utils - INFO - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpjfpdu9cp\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:38,271 - transformers.file_utils - INFO - storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json in cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:38,271 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:38,272 - filelock - INFO - Lock 140138766067192 released on /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391.lock\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:38,272 - transformers.configuration_utils - INFO - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:38,272 - transformers.configuration_utils - INFO - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:38,481 - filelock - INFO - Lock 140138765558896 acquired on /root/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:38,481 - transformers.file_utils - INFO - https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp8omvpehy\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:46,735 - transformers.file_utils - INFO - storing https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin in cache at /root/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:46,735 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:46,736 - filelock - INFO - Lock 140138765558896 released on /root/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:46,736 - transformers.modeling_utils - INFO - loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:49,875 - transformers.modeling_utils - WARNING - Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:49,875 - transformers.modeling_utils - WARNING - Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m2020-10-02 01:47:49,940 - trainer - INFO - Using multi gpu with devices ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'], default cuda:0 \u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mNCCL version 2.4.8+cuda10.1\u001b[0m\n",
      "\u001b[34m2020-10-02 01:48:29,836 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-02 01:48:35,439 - trainer - INFO - Train set result details: 0.7402868318122555\u001b[0m\n",
      "\u001b[34m2020-10-02 01:48:35,440 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-02 01:49:49,241 - trainer - INFO - Validation set result details: 0.624821016334999 \u001b[0m\n",
      "\u001b[34m2020-10-02 01:49:49,241 - trainer - INFO - Snapshotting because the current score 0.624821016334999 is greater than None \u001b[0m\n",
      "\u001b[34m2020-10-02 01:49:49,241 - trainer - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2020-10-02 01:49:49,646 - trainer - INFO - Checkpoint model to /opt/ml/checkpoints/checkpoint.pt\u001b[0m\n",
      "\u001b[34m2020-10-02 01:49:50,096 - trainer - INFO - Run    120     0        94    25/94         27% 0.001027 0.000370       0.7403       0.6248\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.0010273534129002192\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.00036956414415423446\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.7402868318122555\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.624821016334999\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-10-02 01:50:23,412 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-02 01:50:29,004 - trainer - INFO - Train set result details: 0.7545622851097593\u001b[0m\n",
      "\u001b[34m2020-10-02 01:50:29,004 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2020-10-02 01:51:43,927 - trainer - INFO - Validation set result details: 0.633512924511169 \u001b[0m\n",
      "\u001b[34m2020-10-02 01:51:43,982 - trainer - INFO - Snapshotting because the current score 0.633512924511169 is greater than 0.624821016334999 \u001b[0m\n",
      "\u001b[34m2020-10-02 01:51:43,982 - trainer - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2020-10-02 01:51:44,452 - trainer - INFO - Run    234     1       188    25/94         27% 0.000959 0.000358       0.7546       0.6335\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.0009593281531433991\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.00035822014991814894\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.7545622851097593\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.633512924511169\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-10-02 01:52:17,513 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2020-10-02 01:52:23,292 - trainer - INFO - Train set result details: 0.768260641093011\u001b[0m\n",
      "\u001b[34m2020-10-02 01:52:23,292 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='main.py',\n",
    "                    source_dir = 'src',\n",
    "                    role=role,\n",
    "                    framework_version =\"1.4.0\",\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type=instance_type,\n",
    "                    hyperparameters = hp,\n",
    "                    output_path=s3_output_path,\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    volume_size=30,\n",
    "                    code_location=s3_code_path,\n",
    "                    debugger_hook_config=False,\n",
    "                    base_job_name =base_name,  \n",
    "                    use_spot_instances = use_spot,\n",
    "                    max_run =  train_max_run_secs,\n",
    "                    max_wait = max_wait_time_secs,   \n",
    "                    checkpoint_s3_uri=s3_checkpoint,\n",
    "                    checkpoint_local_path=sm_localcheckpoint_dir\n",
    "                    )\n",
    "\n",
    "estimator.fit(inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference container\n",
    "Ideally the server containing should already have all the required dependencies installed to reduce start up time and ensure that the runtime enviornment is consistent. This can be implemented using a custom docker image.\n",
    "\n",
    "But for this demo, to simplify, we will let the Pytorch container script model install the dependencies during start up. As a result, you will see some of the initial ping requests fail, until all dependencies are installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "model_uri = estimator.model_data\n",
    "\n",
    "model = PyTorchModel(model_data=model_uri,\n",
    "                     role=role,\n",
    "                     framework_version='1.4.0',\n",
    "                     entry_point='serve.py',\n",
    "                     source_dir='src')\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Q-workshop is a Polish company located in Poznań that specializes in designand production of polyhedral dice\",\n",
    "        \"ET is a sci-fi directed by steven spielberg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.content_types import CONTENT_TYPE_CSV\n",
    "import json\n",
    "\n",
    "predictor.content_type = CONTENT_TYPE_CSV\n",
    "predictor.serializer=None\n",
    "predictor.deserializer = None\n",
    "\n",
    "\n",
    "data_bytes=\"\\n\".join(data).encode(\"utf-8\")\n",
    "response_bytes  = predictor.predict(data_bytes,  \n",
    "                                    initial_args={ \"Accept\":\"text/json\", \"ContentType\" : CONTENT_TYPE_CSV }\n",
    "                                   )\n",
    "\n",
    "json.loads(response_bytes.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
