{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chemprot: Bert NER on SageMaker using PyTorch\n",
    "\n",
    "This uses the chemprot chemical protien names corpus in https://biocreative.bioinformatics.udel.edu/news/corpora/chemprot-corpus-biocreative-vi/\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import logging\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "logging.basicConfig(level=\"INFO\", handlers=[logging.StreamHandler(sys.stdout)],\n",
    "                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket and role set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "#from sagemaker import get_execution_role\n",
    "\n",
    "import sagemaker.session \n",
    "sm_session = sagemaker.session.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# role=get_execution_role()\n",
    "role =\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20181222T162635\".format(account_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = \"aegovan-data\"\n",
    "\n",
    "data_bucket_prefix = \"chemprotner\"\n",
    "\n",
    "s3_uri_data = \"s3://{}/{}\".format(data_bucket, data_bucket_prefix)\n",
    "s3_uri_train = \"{}/{}\".format(s3_uri_data, \"train/chemprot_training_abstracts.tsv\")\n",
    "s3_uri_train_classes = \"{}/{}\".format(s3_uri_data, \"train/chemprot_training_entities.tsv\")\n",
    "\n",
    "s3_uri_val = \"{}/{}\".format(s3_uri_data, \"val/chemprot_development_abstracts.tsv\")\n",
    "s3_uri_val_classes = \"{}/{}\".format(s3_uri_data, \"val/chemprot_development_entities.tsv\")\n",
    "\n",
    "\n",
    "\n",
    "s3_uri_test = \"s3://{}/{}/test data\".format(data_bucket, data_bucket_prefix , \"test.in\")\n",
    "\n",
    "s3_output_path = \"s3://{}/{}/output\".format(data_bucket, data_bucket_prefix)\n",
    "s3_code_path = \"s3://{}/{}/code\".format(data_bucket, data_bucket_prefix)\n",
    "s3_checkpoint = \"s3://{}/{}/checkpoint\".format(data_bucket, data_bucket_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "This shows you how to train BERT on SageMaker using SPOT instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_full =  {\n",
    "    \"train\" : s3_uri_train,\n",
    "    \"class\" : s3_uri_train_classes,\n",
    "    \"val\" : s3_uri_val,\n",
    "    \"valclass\" : s3_uri_val_classes\n",
    "}\n",
    "\n",
    "inputs = inputs_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_localcheckpoint_dir=\"/opt/ml/checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p3.2xlarge\"\n",
    "instance_type_gpu_map = {\"ml.p3.8xlarge\":4, \"ml.p3.2xlarge\": 1, \"ml.p3.16xlarge\":8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\n",
    "\"epochs\" : 30,\n",
    "\"earlystoppingpatience\" : 5,\n",
    "# Increasing batch size might end up with CUDA OOM error, increase grad accumulation instead\n",
    "\"batch\" : 8 * instance_type_gpu_map[instance_type],\n",
    "\"trainfile\" :s3_uri_train.split(\"/\")[-1],\n",
    "\"classfile\":s3_uri_train_classes.split(\"/\")[-1],\n",
    "\"valfile\" :s3_uri_val.split(\"/\")[-1],\n",
    "\"valclassfile\":s3_uri_val_classes.split(\"/\")[-1],\n",
    "\"datasetfactory\":\"datasets.chemprot_dataset_factory.ChemprotDatasetFactory\",\n",
    "# The number of steps to accumulate gradients for\n",
    "\"gradaccumulation\" : 4,\n",
    "\"log-level\":\"INFO\",\n",
    "# This param depends on your model max pos embedding size or when large you might end up with CUDA OOM error    \n",
    "\"maxseqlen\" : 512,\n",
    "# Make sure the lr is quite small, as this is a pretrained model..\n",
    "\"lr\":0.00001,\n",
    "# Use finetuning (set to 1), if you only want to change the weights in the final classification layer.. \n",
    "\"finetune\": 0,\n",
    "\"checkpointdir\" : sm_localcheckpoint_dir,\n",
    "# Checkpoints once every n epochs\n",
    "\"checkpointfreq\": 2,\n",
    "\"log-level\" : \"INFO\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 30,\n",
       " 'earlystoppingpatience': 5,\n",
       " 'batch': 8,\n",
       " 'trainfile': 'chemprot_training_abstracts.tsv',\n",
       " 'classfile': 'chemprot_training_entities.tsv',\n",
       " 'valfile': 'chemprot_development_abstracts.tsv',\n",
       " 'valclassfile': 'chemprot_development_entities.tsv',\n",
       " 'datasetfactory': 'datasets.chemprot_dataset_factory.ChemprotDatasetFactory',\n",
       " 'gradaccumulation': 4,\n",
       " 'log-level': 'INFO',\n",
       " 'maxseqlen': 512,\n",
       " 'lr': 1e-05,\n",
       " 'finetune': 0,\n",
       " 'checkpointdir': '/opt/ml/checkpoints/',\n",
       " 'checkpointfreq': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://aegovan-data/chemprotner/train/chemprot_training_abstracts.tsv',\n",
       " 'class': 's3://aegovan-data/chemprotner/train/chemprot_training_entities.tsv',\n",
       " 'val': 's3://aegovan-data/chemprotner/val/chemprot_development_abstracts.tsv',\n",
       " 'valclass': 's3://aegovan-data/chemprotner/val/chemprot_development_entities.tsv'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{\"Name\": \"TrainLoss\",\n",
    "                     \"Regex\": \"###score: train_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"ValidationLoss\",\n",
    "                     \"Regex\": \"###score: val_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"TrainScore\",\n",
    "                     \"Regex\": \"###score: train_score### (\\d*[.]?\\d*)\"}\n",
    "                   ,{\"Name\": \"ValidationScore\",\n",
    "                     \"Regex\": \"###score: val_score### (\\d*[.]?\\d*)\"}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True if you need spot instance\n",
    "use_spot = True\n",
    "train_max_run_secs =   2*24 * 60 * 60\n",
    "spot_wait_sec =  5 * 60\n",
    "max_wait_time_secs = train_max_run_secs +  spot_wait_sec\n",
    "\n",
    "if not use_spot:\n",
    "    max_wait_time_secs = None\n",
    "    \n",
    "# During local mode, no spot.., use smaller dataset\n",
    "if instance_type == 'local':\n",
    "    use_spot = False\n",
    "    max_wait_time_secs = 0\n",
    "    wait = True\n",
    "    # Use smaller dataset to run locally\n",
    "    inputs = inputs_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type = \"chemprot-ner-bert\"\n",
    "base_name = \"{}\".format(job_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-16 13:37:04,955 - sagemaker.image_uris - INFO - Defaulting to the only supported framework/algorithm version: latest.\n",
      "2022-07-16 13:37:04,981 - sagemaker.image_uris - INFO - Ignoring unnecessary instance type: None.\n",
      "2022-07-16 13:37:05,000 - sagemaker - INFO - Creating training-job with name: chemprot-ner-bert-2022-07-16-18-37-04-135\n",
      "2022-07-16 18:37:05 Starting - Starting the training job...\n",
      "2022-07-16 18:37:30 Starting - Preparing the instances for trainingProfilerReport-1657996624: InProgress\n",
      "............\n",
      "2022-07-16 18:39:40 Downloading - Downloading input data\n",
      "2022-07-16 18:39:40 Training - Downloading the training image............\n",
      "2022-07-16 18:41:41 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-07-16 18:41:44,135 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-07-16 18:41:44,160 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-07-16 18:41:44,164 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-07-16 18:41:44,530 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2022-07-16 18:41:44,531 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2022-07-16 18:41:44,531 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2022-07-16 18:41:44,531 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpl0bmf8qg/module_dir\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.4.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.4.0)\u001b[0m\n",
      "\u001b[34mCollecting transformers==3.0.1\n",
      "  Downloading transformers-3.0.1-py3-none-any.whl (757 kB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==0.23.1\n",
      "  Downloading scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting seqeval==0.0.17\n",
      "  Downloading seqeval-0.0.17.tar.gz (20 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.23.1->-r requirements.txt (line 3)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.23.1->-r requirements.txt (line 3)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.23.1->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting Keras>=2.2.4\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (4.56.0)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.96-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.7.9-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.1->-r requirements.txt (line 2)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.4.1-py3-none-any.whl (9.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.8.0-rc4\n",
      "  Downloading tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==3.0.1->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==3.0.1->-r requirements.txt (line 2)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==3.0.1->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==3.0.1->-r requirements.txt (line 2)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==3.0.1->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==3.0.1->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==3.0.1->-r requirements.txt (line 2)) (7.1.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name, seqeval, sacremoses\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=33516 sha256=bb8e0814d2fa67a78cb1e23d86af9511a21c29cc0ed506e1a4e6c87629487658\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lqhnbb5a/wheels/a5/4b/3d/72f6d8f4950ce2c0cdd1de2627c9ab13c44de00c32c66aa0a2\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.17-py3-none-any.whl size=7637 sha256=f35b0a69b6abdbf9d0be03f1a01c9125db60787d5170d7fde5e925e193b52c7d\n",
      "  Stored in directory: /root/.cache/pip/wheels/a0/8f/33/b734af42c4a11d13fa4412f5019cde239430c25ca654f29536\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895254 sha256=c4a7d71bdc96accecd312a5643561b6850188a0c4a3e0e1309f66b9c4bcb4e8c\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/64/31/e9900a234b23fb3e9dc565d6114a9d6ff84a72dbdd356502b4\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name seqeval sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, tokenizers, threadpoolctl, sentencepiece, sacremoses, Keras, filelock, transformers, seqeval, scikit-learn, default-user-module-name\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.21.2\n",
      "    Uninstalling scikit-learn-0.21.2:\n",
      "      Successfully uninstalled scikit-learn-0.21.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed Keras-2.9.0 default-user-module-name-1.0.0 filelock-3.4.1 regex-2022.7.9 sacremoses-0.0.53 scikit-learn-0.23.1 sentencepiece-0.1.96 seqeval-0.0.17 threadpoolctl-3.1.0 tokenizers-0.8.0rc4 transformers-3.0.1\u001b[0m\n",
      "\u001b[34m2022-07-16 18:41:58,065 sagemaker-containers INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"class\": \"/opt/ml/input/data/class\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\",\n",
      "        \"valclass\": \"/opt/ml/input/data/valclass\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch\": 8,\n",
      "        \"checkpointdir\": \"/opt/ml/checkpoints/\",\n",
      "        \"checkpointfreq\": 2,\n",
      "        \"classfile\": \"chemprot_training_entities.tsv\",\n",
      "        \"datasetfactory\": \"datasets.chemprot_dataset_factory.ChemprotDatasetFactory\",\n",
      "        \"earlystoppingpatience\": 5,\n",
      "        \"epochs\": 30,\n",
      "        \"finetune\": 0,\n",
      "        \"gradaccumulation\": 4,\n",
      "        \"log-level\": \"INFO\",\n",
      "        \"lr\": 1e-05,\n",
      "        \"maxseqlen\": 512,\n",
      "        \"trainfile\": \"chemprot_training_abstracts.tsv\",\n",
      "        \"valclassfile\": \"chemprot_development_entities.tsv\",\n",
      "        \"valfile\": \"chemprot_development_abstracts.tsv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"class\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"valclass\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"chemprot-ner-bert-2022-07-16-18-37-04-135\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://aegovan-data/chemprotner/code/chemprot-ner-bert-2022-07-16-18-37-04-135/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"main\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"main.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch\":8,\"checkpointdir\":\"/opt/ml/checkpoints/\",\"checkpointfreq\":2,\"classfile\":\"chemprot_training_entities.tsv\",\"datasetfactory\":\"datasets.chemprot_dataset_factory.ChemprotDatasetFactory\",\"earlystoppingpatience\":5,\"epochs\":30,\"finetune\":0,\"gradaccumulation\":4,\"log-level\":\"INFO\",\"lr\":1e-05,\"maxseqlen\":512,\"trainfile\":\"chemprot_training_abstracts.tsv\",\"valclassfile\":\"chemprot_development_entities.tsv\",\"valfile\":\"chemprot_development_abstracts.tsv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=main.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"class\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valclass\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"class\",\"train\",\"val\",\"valclass\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=main\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://aegovan-data/chemprotner/code/chemprot-ner-bert-2022-07-16-18-37-04-135/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"class\":\"/opt/ml/input/data/class\",\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\",\"valclass\":\"/opt/ml/input/data/valclass\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch\":8,\"checkpointdir\":\"/opt/ml/checkpoints/\",\"checkpointfreq\":2,\"classfile\":\"chemprot_training_entities.tsv\",\"datasetfactory\":\"datasets.chemprot_dataset_factory.ChemprotDatasetFactory\",\"earlystoppingpatience\":5,\"epochs\":30,\"finetune\":0,\"gradaccumulation\":4,\"log-level\":\"INFO\",\"lr\":1e-05,\"maxseqlen\":512,\"trainfile\":\"chemprot_training_abstracts.tsv\",\"valclassfile\":\"chemprot_development_entities.tsv\",\"valfile\":\"chemprot_development_abstracts.tsv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"class\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valclass\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"chemprot-ner-bert-2022-07-16-18-37-04-135\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://aegovan-data/chemprotner/code/chemprot-ner-bert-2022-07-16-18-37-04-135/source/sourcedir.tar.gz\",\"module_name\":\"main\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"main.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch\",\"8\",\"--checkpointdir\",\"/opt/ml/checkpoints/\",\"--checkpointfreq\",\"2\",\"--classfile\",\"chemprot_training_entities.tsv\",\"--datasetfactory\",\"datasets.chemprot_dataset_factory.ChemprotDatasetFactory\",\"--earlystoppingpatience\",\"5\",\"--epochs\",\"30\",\"--finetune\",\"0\",\"--gradaccumulation\",\"4\",\"--log-level\",\"INFO\",\"--lr\",\"1e-05\",\"--maxseqlen\",\"512\",\"--trainfile\",\"chemprot_training_abstracts.tsv\",\"--valclassfile\",\"chemprot_development_entities.tsv\",\"--valfile\",\"chemprot_development_abstracts.tsv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CLASS=/opt/ml/input/data/class\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALCLASS=/opt/ml/input/data/valclass\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH=8\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINTDIR=/opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINTFREQ=2\u001b[0m\n",
      "\u001b[34mSM_HP_CLASSFILE=chemprot_training_entities.tsv\u001b[0m\n",
      "\u001b[34mSM_HP_DATASETFACTORY=datasets.chemprot_dataset_factory.ChemprotDatasetFactory\u001b[0m\n",
      "\u001b[34mSM_HP_EARLYSTOPPINGPATIENCE=5\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=30\u001b[0m\n",
      "\u001b[34mSM_HP_FINETUNE=0\u001b[0m\n",
      "\u001b[34mSM_HP_GRADACCUMULATION=4\u001b[0m\n",
      "\u001b[34mSM_HP_LOG-LEVEL=INFO\u001b[0m\n",
      "\u001b[34mSM_HP_LR=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MAXSEQLEN=512\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINFILE=chemprot_training_abstracts.tsv\u001b[0m\n",
      "\u001b[34mSM_HP_VALCLASSFILE=chemprot_development_entities.tsv\u001b[0m\n",
      "\u001b[34mSM_HP_VALFILE=chemprot_development_abstracts.tsv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 main.py --batch 8 --checkpointdir /opt/ml/checkpoints/ --checkpointfreq 2 --classfile chemprot_training_entities.tsv --datasetfactory datasets.chemprot_dataset_factory.ChemprotDatasetFactory --earlystoppingpatience 5 --epochs 30 --finetune 0 --gradaccumulation 4 --log-level INFO --lr 1e-05 --maxseqlen 512 --trainfile chemprot_training_abstracts.tsv --valclassfile chemprot_development_entities.tsv --valfile chemprot_development_abstracts.tsv\u001b[0m\n",
      "\u001b[34m{'trainfile': 'chemprot_training_abstracts.tsv', 'traindir': '/opt/ml/input/data/train', 'classfile': 'chemprot_training_entities.tsv', 'classdir': '/opt/ml/input/data/class', 'valfile': 'chemprot_development_abstracts.tsv', 'valdir': '/opt/ml/input/data/val', 'valclassfile': 'chemprot_development_entities.tsv', 'valclassdir': '/opt/ml/input/data/valclass', 'datasetfactory': 'datasets.chemprot_dataset_factory.ChemprotDatasetFactory', 'outdir': '/opt/ml/output/data', 'modeldir': '/opt/ml/model', 'checkpointdir': '/opt/ml/checkpoints/', 'checkpointfreq': '2', 'earlystoppingpatience': 5, 'epochs': 30, 'gradaccumulation': 4, 'batch': 8, 'lr': 1e-05, 'finetune': 0, 'maxseqlen': 512, 'log_level': 'INFO'}\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:00,509 - transformers.file_utils - INFO - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmphujcq2sa\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:00,653 - transformers.file_utils - INFO - storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt in cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:00,653 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:00,653 - transformers.tokenization_utils_base - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:00,809 - transformers.tokenization_utils_base - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:00,981 - transformers.tokenization_utils_base - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:01,157 - transformers.file_utils - INFO - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpcdtreqh8\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:01,240 - transformers.file_utils - INFO - storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json in cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:01,240 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:01,241 - transformers.configuration_utils - INFO - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:01,241 - transformers.configuration_utils - INFO - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:01,528 - transformers.file_utils - INFO - https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpie2txfba\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2022-07-16 18:42:09,780 - transformers.file_utils - INFO - storing https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin in cache at /root/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:09,780 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:09,780 - transformers.modeling_utils - INFO - loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:13,017 - transformers.modeling_utils - WARNING - Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:13,017 - transformers.modeling_utils - WARNING - Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:48,044 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:49,761 - trainer - INFO - Train set result details: 0.009591392521545242\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:49,761 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:57,355 - trainer - INFO - Validation set result details: 0.0454103146286085 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:57,355 - trainer - INFO - Snapshotting because the current score 0.0454103146286085 is greater than None \u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:57,355 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:57,356 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:57,794 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:57,794 - trainer - INFO - Saving model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:57,795 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:58,244 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:42:58,245 - trainer - INFO - Run     45     0       128     5/128         4% 0.213533 0.060642       0.0096       0.0454\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.21353330253623426\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.06064228640467513\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.009591392521545242\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.0454103146286085\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:43:30,650 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:43:32,573 - trainer - INFO - Train set result details: 0.17307048717907852\u001b[0m\n",
      "\u001b[34m2022-07-16 18:43:32,573 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:43:40,178 - trainer - INFO - Validation set result details: 0.3178999369350431 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:43:40,181 - trainer - INFO - Snapshotting because the current score 0.3178999369350431 is greater than 0.0454103146286085 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:43:40,181 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:43:40,182 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:43:42,684 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:43:42,685 - trainer - INFO - Run     89     1       256     5/128         4% 0.101437 0.041875       0.1731       0.3179\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.1014369239564985\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.04187479062505018\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.17307048717907852\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.3178999369350431\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:44:15,257 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:44:17,056 - trainer - INFO - Train set result details: 0.3493306141245354\u001b[0m\n",
      "\u001b[34m2022-07-16 18:44:17,056 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:44:24,672 - trainer - INFO - Validation set result details: 0.4259034027946701 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:44:24,675 - trainer - INFO - Snapshotting because the current score 0.4259034027946701 is greater than 0.3178999369350431 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:44:24,675 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:44:24,676 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:44:27,206 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:44:27,206 - trainer - INFO - Run    134     2       384     5/128         4% 0.074239 0.035636       0.3493       0.4259\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.07423946011113003\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.035635669514829035\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.3493306141245354\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.4259034027946701\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:44:59,911 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:01,697 - trainer - INFO - Train set result details: 0.4437640334762196\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:01,697 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:09,440 - trainer - INFO - Validation set result details: 0.49389927664801414 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:09,443 - trainer - INFO - Snapshotting because the current score 0.49389927664801414 is greater than 0.4259034027946701 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:09,443 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:09,444 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:11,979 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:11,979 - trainer - INFO - Saving model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:11,980 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:15,134 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:15,135 - trainer - INFO - Run    182     3       512     5/128         4% 0.060882 0.032586       0.4438       0.4939\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.06088196614291519\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.03258593217317575\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.4437640334762196\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.49389927664801414\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:48,033 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:49,777 - trainer - INFO - Train set result details: 0.49898328450801316\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:49,777 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:57,442 - trainer - INFO - Validation set result details: 0.5256564413746819 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:57,445 - trainer - INFO - Snapshotting because the current score 0.5256564413746819 is greater than 0.49389927664801414 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:57,445 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:57,446 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:59,965 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:45:59,965 - trainer - INFO - Run    226     4       640     5/128         4% 0.053340 0.030168       0.4990       0.5257\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.05334033504186664\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.030168144214971392\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.49898328450801316\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.5256564413746819\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:46:32,887 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:46:34,831 - trainer - INFO - Train set result details: 0.5416089859288206\u001b[0m\n",
      "\u001b[34m2022-07-16 18:46:34,831 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:46:42,436 - trainer - INFO - Validation set result details: 0.5557880450149861 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:46:42,439 - trainer - INFO - Snapshotting because the current score 0.5557880450149861 is greater than 0.5256564413746819 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:46:42,439 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:46:42,440 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:46:45,080 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:46:45,080 - trainer - INFO - Run    272     5       768     5/128         4% 0.046790 0.028724       0.5416       0.5558\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.046789617466856726\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.028724323422495836\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.5416089859288206\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.5557880450149861\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:47:18,025 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:47:19,865 - trainer - INFO - Train set result details: 0.5775412378401241\u001b[0m\n",
      "\u001b[34m2022-07-16 18:47:19,865 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:47:27,532 - trainer - INFO - Validation set result details: 0.5742674871563258 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:47:27,535 - trainer - INFO - Snapshotting because the current score 0.5742674871563258 is greater than 0.5557880450149861 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:47:27,536 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:47:27,537 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:47:30,064 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:47:30,064 - trainer - INFO - Saving model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2022-07-16 18:47:30,065 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:47:33,225 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:47:33,225 - trainer - INFO - Run    320     6       896     5/128         4% 0.041987 0.027585       0.5775       0.5743\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.04198712467041332\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.027584534454969018\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.5775412378401241\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.5742674871563258\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:48:06,135 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:48:07,898 - trainer - INFO - Train set result details: 0.6100242753105812\u001b[0m\n",
      "\u001b[34m2022-07-16 18:48:07,898 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:48:15,573 - trainer - INFO - Validation set result details: 0.5895569080257095 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:48:15,575 - trainer - INFO - Snapshotting because the current score 0.5895569080257095 is greater than 0.5742674871563258 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:48:15,576 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:48:15,576 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:48:18,089 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:48:18,089 - trainer - INFO - Run    365     7      1024     5/128         4% 0.037896 0.026472       0.6100       0.5896\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.03789641296316404\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.026471931511670155\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.6100242753105812\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.5895569080257095\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:48:51,077 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:48:52,856 - trainer - INFO - Train set result details: 0.6374950541347433\u001b[0m\n",
      "\u001b[34m2022-07-16 18:48:52,856 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:00,542 - trainer - INFO - Validation set result details: 0.606280538331857 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:00,545 - trainer - INFO - Snapshotting because the current score 0.606280538331857 is greater than 0.5895569080257095 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:00,545 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:00,546 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:03,063 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:03,063 - trainer - INFO - Run    410     8      1152     5/128         4% 0.034175 0.026417       0.6375       0.6063\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.03417532576713711\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.02641656722118652\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.6374950541347433\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.606280538331857\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:36,006 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:37,828 - trainer - INFO - Train set result details: 0.6611372257209749\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:37,828 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:45,517 - trainer - INFO - Validation set result details: 0.614081738828834 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:45,520 - trainer - INFO - Snapshotting because the current score 0.614081738828834 is greater than 0.606280538331857 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:45,520 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:45,521 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:48,041 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:48,042 - trainer - INFO - Saving model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:48,042 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:51,199 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:49:51,200 - trainer - INFO - Run    458     9      1280     5/128         4% 0.031603 0.025357       0.6611       0.6141\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.0316032511764206\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.02535685055639619\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.6611372257209749\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.614081738828834\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:50:24,145 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:50:25,889 - trainer - INFO - Train set result details: 0.6834308205324442\u001b[0m\n",
      "\u001b[34m2022-07-16 18:50:25,889 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:50:33,591 - trainer - INFO - Validation set result details: 0.6289976202913693 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:50:33,594 - trainer - INFO - Snapshotting because the current score 0.6289976202913693 is greater than 0.614081738828834 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:50:33,594 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:50:33,595 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:50:36,110 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:50:36,110 - trainer - INFO - Run    503    10      1408     5/128         4% 0.028880 0.024896       0.6834       0.6290\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.028880055011541117\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.024896222598899422\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.6834308205324442\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6289976202913693\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:51:09,055 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:51:10,797 - trainer - INFO - Train set result details: 0.6971013695616304\u001b[0m\n",
      "\u001b[34m2022-07-16 18:51:10,797 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:51:18,491 - trainer - INFO - Validation set result details: 0.6404012714706483 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:51:18,494 - trainer - INFO - Snapshotting because the current score 0.6404012714706483 is greater than 0.6289976202913693 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:51:18,494 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:51:18,495 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:51:21,008 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:51:21,009 - trainer - INFO - Run    547    11      1536     5/128         4% 0.026884 0.024360       0.6971       0.6404\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.026883640435698908\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.024360087621056178\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.6971013695616304\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6404012714706483\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:51:53,941 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:51:55,746 - trainer - INFO - Train set result details: 0.7131273204159664\u001b[0m\n",
      "\u001b[34m2022-07-16 18:51:55,746 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:03,572 - trainer - INFO - Validation set result details: 0.6455541312201376 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:03,575 - trainer - INFO - Snapshotting because the current score 0.6455541312201376 is greater than 0.6404012714706483 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:03,575 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:03,576 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:06,109 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:06,109 - trainer - INFO - Saving model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:06,110 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:09,276 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:09,276 - trainer - INFO - Run    596    12      1664     5/128         4% 0.024900 0.025656       0.7131       0.6456\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.02490044179285178\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.025655530831393075\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.7131273204159664\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6455541312201376\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:42,229 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:44,061 - trainer - INFO - Train set result details: 0.731608715808994\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:44,062 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:51,712 - trainer - INFO - Validation set result details: 0.6570893557259054 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:51,715 - trainer - INFO - Snapshotting because the current score 0.6570893557259054 is greater than 0.6455541312201376 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:51,715 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:51,716 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:54,239 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:52:54,239 - trainer - INFO - Run    641    13      1792     5/128         4% 0.022497 0.024446       0.7316       0.6571\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.022497150621347828\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.02444625284303637\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.731608715808994\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6570893557259054\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:53:27,127 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:53:28,965 - trainer - INFO - Train set result details: 0.7444079928422308\u001b[0m\n",
      "\u001b[34m2022-07-16 18:53:28,965 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:53:36,622 - trainer - INFO - Validation set result details: 0.6651828110161443 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:53:36,625 - trainer - INFO - Snapshotting because the current score 0.6651828110161443 is greater than 0.6570893557259054 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:53:36,625 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:53:36,626 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:53:39,138 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:53:39,138 - trainer - INFO - Run    686    14      1920     5/128         4% 0.020832 0.024288       0.7444       0.6652\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.020831632580666337\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.024288011459158917\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.7444079928422308\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6651828110161443\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:54:12,071 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:54:13,834 - trainer - INFO - Train set result details: 0.7555804823331463\u001b[0m\n",
      "\u001b[34m2022-07-16 18:54:13,834 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:54:21,634 - trainer - INFO - Validation set result details: 0.6740935551549895 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:54:21,637 - trainer - INFO - Snapshotting because the current score 0.6740935551549895 is greater than 0.6651828110161443 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:54:21,637 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:54:21,638 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:54:24,158 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:54:24,158 - trainer - INFO - Saving model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2022-07-16 18:54:24,159 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:54:27,315 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:54:27,315 - trainer - INFO - Run    734    15      2048     5/128         4% 0.019562 0.024127       0.7556       0.6741\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.0195624249681714\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.024126945538263694\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.7555804823331463\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6740935551549895\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:00,257 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:02,064 - trainer - INFO - Train set result details: 0.7714076053210907\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:02,064 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:09,771 - trainer - INFO - Validation set result details: 0.6797142342558684 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:09,774 - trainer - INFO - Snapshotting because the current score 0.6797142342558684 is greater than 0.6740935551549895 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:09,774 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:09,775 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:12,299 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:12,299 - trainer - INFO - Run    779    16      2176     5/128         4% 0.018147 0.023546       0.7714       0.6797\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.018147063798096497\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.023545865955598214\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.7714076053210907\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6797142342558684\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:45,218 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:47,137 - trainer - INFO - Train set result details: 0.7796220151646611\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:47,137 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:54,875 - trainer - INFO - Validation set result details: 0.6840360018948366 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:54,878 - trainer - INFO - Snapshotting because the current score 0.6840360018948366 is greater than 0.6797142342558684 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:54,878 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:54,879 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:57,408 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:55:57,409 - trainer - INFO - Run    824    17      2304     5/128         4% 0.016918 0.024510       0.7796       0.6840\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.01691831779316999\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.024510407771647365\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.7796220151646611\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6840360018948366\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:56:30,420 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:56:32,155 - trainer - INFO - Train set result details: 0.7933498004940148\u001b[0m\n",
      "\u001b[34m2022-07-16 18:56:32,155 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:56:39,838 - trainer - INFO - Validation set result details: 0.6848536349517977 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:56:39,841 - trainer - INFO - Snapshotting because the current score 0.6848536349517977 is greater than 0.6840360018948366 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:56:39,841 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:56:39,842 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:56:42,364 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:56:42,364 - trainer - INFO - Saving model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2022-07-16 18:56:42,365 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:56:45,518 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:56:45,519 - trainer - INFO - Run    872    18      2432     5/128         4% 0.015862 0.025123       0.7933       0.6849\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.015862392534472747\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.02512263359343694\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.7933498004940148\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6848536349517977\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:57:18,427 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:57:20,197 - trainer - INFO - Train set result details: 0.7981086931753447\u001b[0m\n",
      "\u001b[34m2022-07-16 18:57:20,198 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:57:27,905 - trainer - INFO - Validation set result details: 0.6936124142908697 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:57:27,908 - trainer - INFO - Snapshotting because the current score 0.6936124142908697 is greater than 0.6848536349517977 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:57:27,908 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:57:27,909 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:57:30,435 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:57:30,435 - trainer - INFO - Run    917    19      2560     5/128         4% 0.014893 0.024615       0.7981       0.6936\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.01489260947710136\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.024614746773652003\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.7981086931753447\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6936124142908697\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:03,352 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:05,237 - trainer - INFO - Train set result details: 0.8080507747727144\u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:05,237 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:12,924 - trainer - INFO - Validation set result details: 0.6949675178876149 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:12,927 - trainer - INFO - Snapshotting because the current score 0.6949675178876149 is greater than 0.6936124142908697 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:12,927 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:12,928 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:15,443 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:15,444 - trainer - INFO - Run    962    20      2688     5/128         4% 0.014124 0.024666       0.8081       0.6950\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.014124262852419633\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.02466585008683158\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.8080507747727144\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.6949675178876149\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:48,383 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:50,205 - trainer - INFO - Train set result details: 0.8145823366185055\u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:50,206 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:57,953 - trainer - INFO - Validation set result details: 0.7004051030896669 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:57,956 - trainer - INFO - Snapshotting because the current score 0.7004051030896669 is greater than 0.6949675178876149 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:57,956 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:58:57,957 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:00,476 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:00,476 - trainer - INFO - Saving model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:00,477 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:03,634 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:03,635 - trainer - INFO - Run   1010    21      2816     5/128         4% 0.013365 0.024172       0.8146       0.7004\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.013365475191676524\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.02417190033285057\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.8145823366185055\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.7004051030896669\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:36,606 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:38,502 - trainer - INFO - Train set result details: 0.8210812466498201\u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:38,502 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:46,189 - trainer - INFO - Validation set result details: 0.7039190288279061 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:46,192 - trainer - INFO - Snapshotting because the current score 0.7039190288279061 is greater than 0.7004051030896669 \u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:46,192 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:46,193 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:48,708 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 18:59:48,708 - trainer - INFO - Run   1055    22      2944     5/128         4% 0.012784 0.025073       0.8211       0.7039\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.012783791869878769\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.025072723444478185\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.8210812466498201\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.7039190288279061\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:00:21,614 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 19:00:23,358 - trainer - INFO - Train set result details: 0.8287254657431745\u001b[0m\n",
      "\u001b[34m2022-07-16 19:00:23,359 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:00:31,037 - trainer - INFO - Validation set result details: 0.7046261222298446 \u001b[0m\n",
      "\u001b[34m2022-07-16 19:00:31,041 - trainer - INFO - Snapshotting because the current score 0.7046261222298446 is greater than 0.7039190288279061 \u001b[0m\n",
      "\u001b[34m2022-07-16 19:00:31,041 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 19:00:31,042 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 19:00:33,605 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 19:00:33,606 - trainer - INFO - Run   1100    23      3072     5/128         4% 0.012118 0.025482       0.8287       0.7046\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.012118382031985675\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.02548171951149414\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.8287254657431745\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.7046261222298446\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:06,449 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:08,308 - trainer - INFO - Train set result details: 0.8358444761280877\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:08,308 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:15,984 - trainer - INFO - Validation set result details: 0.7118038740920097 \u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:15,987 - trainer - INFO - Snapshotting because the current score 0.7118038740920097 is greater than 0.7046261222298446 \u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:15,987 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:15,988 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:18,517 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:18,517 - trainer - INFO - Saving model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:18,518 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:21,694 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:21,695 - trainer - INFO - Run   1148    24      3200     5/128         4% 0.011582 0.024975       0.8358       0.7118\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.011582221290154848\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.024974535274155\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.8358444761280877\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.7118038740920097\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:54,603 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:56,405 - trainer - INFO - Train set result details: 0.8404814636494945\u001b[0m\n",
      "\u001b[34m2022-07-16 19:01:56,405 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:02:04,176 - trainer - INFO - Validation set result details: 0.7107050549978963 \u001b[0m\n",
      "\u001b[34m2022-07-16 19:02:04,176 - trainer - INFO - Run   1191    25      3328     5/128         4% 0.011055 0.025358       0.8405       0.7107\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.011054797074393719\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.02535785482648541\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.8404814636494945\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.7107050549978963\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:02:37,071 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 19:02:38,839 - trainer - INFO - Train set result details: 0.8456622062600166\u001b[0m\n",
      "\u001b[34m2022-07-16 19:02:38,839 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:02:46,531 - trainer - INFO - Validation set result details: 0.7142424792688095 \u001b[0m\n",
      "\u001b[34m2022-07-16 19:02:46,534 - trainer - INFO - Snapshotting because the current score 0.7142424792688095 is greater than 0.7118038740920097 \u001b[0m\n",
      "\u001b[34m2022-07-16 19:02:46,534 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 19:02:46,535 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 19:02:49,050 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 19:02:49,050 - trainer - INFO - Run   1236    26      3456     5/128         4% 0.010710 0.025366       0.8457       0.7142\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.010710178943554638\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.025365718439513563\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.8456622062600166\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.7142424792688095\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:03:21,947 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 19:03:23,697 - trainer - INFO - Train set result details: 0.8500590318772135\u001b[0m\n",
      "\u001b[34m2022-07-16 19:03:23,697 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:03:31,405 - trainer - INFO - Validation set result details: 0.7136281041792855 \u001b[0m\n",
      "\u001b[34m2022-07-16 19:03:31,405 - trainer - INFO - Saving model to /opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34m2022-07-16 19:03:31,406 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 19:03:33,941 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 19:03:33,941 - trainer - INFO - Run   1280    27      3584     5/128         4% 0.009959 0.025949       0.8501       0.7136\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.009958567712601507\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.025948951714763455\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.8500590318772135\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.7136281041792855\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:04:06,985 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 19:04:08,799 - trainer - INFO - Train set result details: 0.852176606081722\u001b[0m\n",
      "\u001b[34m2022-07-16 19:04:08,799 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:04:16,483 - trainer - INFO - Validation set result details: 0.7187026371627766 \u001b[0m\n",
      "\u001b[34m2022-07-16 19:04:16,486 - trainer - INFO - Snapshotting because the current score 0.7187026371627766 is greater than 0.7142424792688095 \u001b[0m\n",
      "\u001b[34m2022-07-16 19:04:16,486 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 19:04:16,487 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 19:04:19,003 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 19:04:19,003 - trainer - INFO - Run   1325    28      3712     5/128         4% 0.009720 0.026447       0.8522       0.7187\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.00972012793954491\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.026446931405005113\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.852176606081722\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.7187026371627766\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:04:51,916 - trainer - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2022-07-16 19:04:53,770 - trainer - INFO - Train set result details: 0.8591018568875767\u001b[0m\n",
      "\u001b[34m2022-07-16 19:04:53,770 - trainer - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mThe current process just got forked. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m2022-07-16 19:05:01,477 - trainer - INFO - Validation set result details: 0.7238136086516956 \u001b[0m\n",
      "\u001b[34m2022-07-16 19:05:01,480 - trainer - INFO - Snapshotting because the current score 0.7238136086516956 is greater than 0.7187026371627766 \u001b[0m\n",
      "\u001b[34m2022-07-16 19:05:01,481 - trainer - INFO - Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-07-16 19:05:01,481 - transformers.configuration_utils - INFO - Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m2022-07-16 19:05:04,009 - transformers.modeling_utils - INFO - Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-07-16 19:05:04,009 - trainer - INFO - Run   1370    29      3840     5/128         4% 0.009240 0.025974       0.8591       0.7238\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.00923954520112602\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.025974152025346663\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.8591018568875767\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.7238136086516956\u001b[0m\n",
      "\u001b[34m2022-07-16 19:05:04,447 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-07-16 19:07:57 Uploading - Uploading generated training model\n",
      "2022-07-16 19:07:57 Completed - Training job completed\n",
      "ProfilerReport-1657996624: IssuesFound\n",
      "Training seconds: 1700\n",
      "Billable seconds: 510\n",
      "Managed Spot Training savings: 70.0%\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='main.py',\n",
    "                    source_dir = 'src',\n",
    "                    role=role,\n",
    "                    framework_version =\"1.4.0\",\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type=instance_type,\n",
    "                    hyperparameters = hp,\n",
    "                    output_path=s3_output_path,\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    volume_size=30,\n",
    "                    code_location=s3_code_path,\n",
    "                    debugger_hook_config=False,\n",
    "                    base_job_name =base_name,  \n",
    "                    use_spot_instances = use_spot,\n",
    "                    max_run =  train_max_run_secs,\n",
    "                    max_wait = max_wait_time_secs,   \n",
    "                    checkpoint_s3_uri=s3_checkpoint,\n",
    "                    checkpoint_local_path=sm_localcheckpoint_dir\n",
    "                    )\n",
    "\n",
    "estimator.fit(inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference container\n",
    "Ideally the server containing should already have all the required dependencies installed to reduce start up time and ensure that the runtime enviornment is consistent. This can be implemented using a custom docker image.\n",
    "\n",
    "But for this demo, to simplify, we will let the Pytorch container script model install the dependencies during start up. As a result, you will see some of the initial ping requests fail, until all dependencies are installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "training_job = \"bc2-ner-bert-2020-10-04-14-09-30-400\"\n",
    "estimator = sagemaker.estimator.Estimator.attach(training_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "role = role\n",
    "\n",
    "model_uri = estimator.model_data\n",
    "\n",
    "model = PyTorchModel(model_data=model_uri,\n",
    "                     role=role,\n",
    "                     framework_version='1.4.0',\n",
    "                     py_version = \"py3\",\n",
    "                     entry_point='serve.py',\n",
    "                     source_dir='src'\n",
    "                    \n",
    "                    )\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    \n",
    "    def serialize(self, x):\n",
    "        return x\n",
    "    \n",
    "    def deserialize(self,x, content_type):\n",
    "        payload_bytes = json.loads( x.read().decode(\"utf-8\") )\n",
    "        return payload_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor= sagemaker.predictor.Predictor(\"end\")\n",
    "predictor.serializer = Predictor()\n",
    "predictor.deserializer = Predictor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from datasets.biocreative_dataset import BiocreativeDataset\n",
    "from datasets.biocreative_ner_label_mapper import BiocreativeNerLabelMapper\n",
    "\n",
    "def chunk(l, size=5):\n",
    "    for i in range(0, len(l),size):\n",
    "        yield l[i:i+size]\n",
    "\n",
    "with open(\"tmp/test.in\", \"r\") as f:\n",
    "    d = []\n",
    "    ids =[]\n",
    "    for l in f.readlines():\n",
    "        l = l.rstrip(\"\\n\")\n",
    "        (id, text) = l.split(\" \")[0], \" \".join(l.split(\" \")[1:])\n",
    "        d.append(text)\n",
    "        ids.append(id)\n",
    "        \n",
    "    \n",
    "    \n",
    "label_mapper = BiocreativeNerLabelMapper()\n",
    "\n",
    "id_chucks = list(chunk(ids))\n",
    "result = []\n",
    "for (i, data) in enumerate(chunk(d)):\n",
    "\n",
    "    data_bytes=(\"\\n\".join(data)).encode(\"utf-8\")\n",
    "    response  = predictor.predict(data_bytes,  \n",
    "                                    initial_args={ \"Accept\":\"text/json\", \"ContentType\" : \"text/csv\" }\n",
    "                                       )\n",
    "\n",
    "    assert len(response) == len(data), \"Data size {} doesnt match result size {}\".format(len(r), len(d))\n",
    "\n",
    "\n",
    "\n",
    "    for ri, r in enumerate(response):\n",
    "        doc_id = id_chucks[i][ri]\n",
    "\n",
    "        result.append({\"docid\":doc_id, \"text\": data[ri], \"entities_detected\": r })\n",
    "        \n",
    "    \n",
    "    \n",
    "with open(\"result.json\", \"w\") as f:\n",
    "    json.dump( result, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
