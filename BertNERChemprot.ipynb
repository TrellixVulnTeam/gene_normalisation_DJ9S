{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chemprot: Bert NER on SageMaker using PyTorch\n",
    "\n",
    "This uses the chemprot chemical protien names corpus in https://biocreative.bioinformatics.udel.edu/news/corpora/chemprot-corpus-biocreative-vi/\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import logging\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "logging.basicConfig(level=\"INFO\", handlers=[logging.StreamHandler(sys.stdout)],\n",
    "                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_temp=\"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $local_temp\n",
    "!mkdir -p $local_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket and role set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-23 14:56:34,348 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2022-07-23 14:56:34,455 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "#from sagemaker import get_execution_role\n",
    "\n",
    "import sagemaker.session \n",
    "sm_session = sagemaker.session.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# role=get_execution_role()\n",
    "role =\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20181222T162635\".format(account_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = \"aegovan-data\"\n",
    "\n",
    "data_bucket_prefix = \"chemprotner\"\n",
    "\n",
    "s3_uri_data = \"s3://{}/{}\".format(data_bucket, data_bucket_prefix)\n",
    "s3_uri_train = \"{}/{}\".format(s3_uri_data, \"train/chemprot_training_abstracts.tsv\")\n",
    "s3_uri_train_classes = \"{}/{}\".format(s3_uri_data, \"train/chemprot_training_entities.tsv\")\n",
    "\n",
    "s3_uri_val = \"{}/{}\".format(s3_uri_data, \"val/chemprot_development_abstracts.tsv\")\n",
    "s3_uri_val_classes = \"{}/{}\".format(s3_uri_data, \"val/chemprot_development_entities.tsv\")\n",
    "\n",
    "\n",
    "\n",
    "s3_uri_test = \"{}/{}\".format(s3_uri_data, \"test/chemprot_test_abstracts_gs.tsv\")\n",
    "s3_uri_test_classes = \"{}/{}\".format(s3_uri_data, \"test/chemprot_test_entities_gs.tsv\")\n",
    "\n",
    "\n",
    "s3_output_path = \"s3://{}/{}/output\".format(data_bucket, data_bucket_prefix)\n",
    "s3_code_path = \"s3://{}/{}/code\".format(data_bucket, data_bucket_prefix)\n",
    "s3_checkpoint = \"s3://{}/{}/checkpoint\".format(data_bucket, data_bucket_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "This shows you how to train BERT on SageMaker using SPOT instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_full =  {\n",
    "    \"train\" : s3_uri_train,\n",
    "    \"class\" : s3_uri_train_classes,\n",
    "    \"val\" : s3_uri_val,\n",
    "    \"valclass\" : s3_uri_val_classes\n",
    "}\n",
    "\n",
    "inputs = inputs_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_localcheckpoint_dir=\"/opt/ml/checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p3.2xlarge\"\n",
    "instance_type_gpu_map = {\"ml.p3.8xlarge\":4, \"ml.p3.2xlarge\": 1, \"ml.p3.16xlarge\":8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\n",
    "\"epochs\" : 50,\n",
    "\"earlystoppingpatience\" : 10,\n",
    "# Increasing batch size might end up with CUDA OOM error, increase grad accumulation instead\n",
    "\"batch\" : 8 * instance_type_gpu_map[instance_type],\n",
    "\"trainfile\" :s3_uri_train.split(\"/\")[-1],\n",
    "\"classfile\":s3_uri_train_classes.split(\"/\")[-1],\n",
    "\"valfile\" :s3_uri_val.split(\"/\")[-1],\n",
    "\"valclassfile\":s3_uri_val_classes.split(\"/\")[-1],\n",
    "\"datasetfactory\":\"datasets.chemprot_dataset_factory.ChemprotDatasetFactory\",\n",
    "# The number of steps to accumulate gradients for\n",
    "\"gradaccumulation\" : 4,\n",
    "\"log-level\":\"INFO\",\n",
    "# This param depends on your model max pos embedding size or when large you might end up with CUDA OOM error    \n",
    "\"maxseqlen\" : 512,\n",
    "# Make sure the lr is quite small, as this is a pretrained model..\n",
    "\"lr\":0.00001,\n",
    "# Use finetuning (set to 1), if you only want to change the weights in the final classification layer.. \n",
    "\"finetune\": 0,\n",
    "\"checkpointdir\" : sm_localcheckpoint_dir,\n",
    "# Checkpoints once every n epochs\n",
    "\"checkpointfreq\": 2,\n",
    "\"log-level\" : \"INFO\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 50,\n",
       " 'earlystoppingpatience': 10,\n",
       " 'batch': 8,\n",
       " 'trainfile': 'chemprot_training_abstracts.tsv',\n",
       " 'classfile': 'chemprot_training_entities.tsv',\n",
       " 'valfile': 'chemprot_development_abstracts.tsv',\n",
       " 'valclassfile': 'chemprot_development_entities.tsv',\n",
       " 'datasetfactory': 'datasets.chemprot_dataset_factory.ChemprotDatasetFactory',\n",
       " 'gradaccumulation': 4,\n",
       " 'log-level': 'INFO',\n",
       " 'maxseqlen': 512,\n",
       " 'lr': 1e-05,\n",
       " 'finetune': 0,\n",
       " 'checkpointdir': '/opt/ml/checkpoints/',\n",
       " 'checkpointfreq': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://aegovan-data/chemprotner/train/chemprot_training_abstracts.tsv',\n",
       " 'class': 's3://aegovan-data/chemprotner/train/chemprot_training_entities.tsv',\n",
       " 'val': 's3://aegovan-data/chemprotner/val/chemprot_development_abstracts.tsv',\n",
       " 'valclass': 's3://aegovan-data/chemprotner/val/chemprot_development_entities.tsv'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{\"Name\": \"TrainLoss\",\n",
    "                     \"Regex\": \"###score: train_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"ValidationLoss\",\n",
    "                     \"Regex\": \"###score: val_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"TrainScore\",\n",
    "                     \"Regex\": \"###score: train_score### (\\d*[.]?\\d*)\"}\n",
    "                   ,{\"Name\": \"ValidationScore\",\n",
    "                     \"Regex\": \"###score: val_score### (\\d*[.]?\\d*)\"}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True if you need spot instance\n",
    "use_spot = False\n",
    "train_max_run_secs =   2*24 * 60 * 60\n",
    "spot_wait_sec =  5 * 60\n",
    "max_wait_time_secs = train_max_run_secs +  spot_wait_sec\n",
    "\n",
    "if not use_spot:\n",
    "    max_wait_time_secs = None\n",
    "    \n",
    "# During local mode, no spot.., use smaller dataset\n",
    "if instance_type == 'local':\n",
    "    use_spot = False\n",
    "    max_wait_time_secs = 0\n",
    "    wait = True\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type = \"chemprot-ner-bert\"\n",
    "base_name = \"{}\".format(job_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='main.py',\n",
    "                    source_dir = 'src',\n",
    "                    role=role,\n",
    "                    framework_version =\"1.4.0\",\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type=instance_type,\n",
    "                    hyperparameters = hp,\n",
    "                    output_path=s3_output_path,\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    volume_size=30,\n",
    "                    code_location=s3_code_path,\n",
    "                    debugger_hook_config=False,\n",
    "                    base_job_name =base_name,  \n",
    "                    use_spot_instances = use_spot,\n",
    "                    max_run =  train_max_run_secs,\n",
    "                    max_wait = max_wait_time_secs,   \n",
    "                    checkpoint_s3_uri=s3_checkpoint,\n",
    "                    checkpoint_local_path=sm_localcheckpoint_dir\n",
    "                    )\n",
    "\n",
    "estimator.fit(inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference container\n",
    "Ideally the server containing should already have all the required dependencies installed to reduce start up time and ensure that the runtime enviornment is consistent. This can be implemented using a custom docker image.\n",
    "\n",
    "But for this demo, to simplify, we will let the Pytorch container script model install the dependencies during start up. As a result, you will see some of the initial ping requests fail, until all dependencies are installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-07-23 22:40:11 Starting - Preparing the instances for training\n",
      "2022-07-23 22:40:11 Downloading - Downloading input data\n",
      "2022-07-23 22:40:11 Training - Training image download completed. Training in progress.\n",
      "2022-07-23 22:40:11 Uploading - Uploading generated training model\n",
      "2022-07-23 22:40:11 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "training_job = \"chemprot-ner-bert-2022-07-23-21-56-34-969\"\n",
    "estimator = sagemaker.estimator.Estimator.attach(training_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-23 17:13:40,137 - sagemaker - INFO - Creating model with name: pytorch-inference-2022-07-24-00-13-40-137\n",
      "2022-07-23 17:13:41,044 - sagemaker - INFO - Creating endpoint-config with name pytorch-inference-2022-07-24-00-13-41-043\n",
      "2022-07-23 17:13:41,193 - sagemaker - INFO - Creating endpoint with name pytorch-inference-2022-07-24-00-13-41-043\n",
      "--------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "role = role\n",
    "\n",
    "model_uri = estimator.model_data\n",
    "\n",
    "model = PyTorchModel(model_data=model_uri,\n",
    "                     role=role,\n",
    "                     framework_version='1.4.0',\n",
    "                     py_version = \"py3\",\n",
    "                     entry_point='serve.py',\n",
    "                     source_dir='src'\n",
    "                    \n",
    "                    )\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    \n",
    "    def serialize(self, x):\n",
    "        return x\n",
    "    \n",
    "    def deserialize(self,x, content_type):\n",
    "        payload_bytes = json.loads( x.read().decode(\"utf-8\") )\n",
    "        return payload_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor= sagemaker.predictor.Predictor(\"end\")\n",
    "predictor.serializer = Predictor()\n",
    "predictor.deserializer = Predictor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_local_dir=local_temp\n",
    "sagemaker.s3.S3Downloader.download(s3_uri_test, test_local_dir)\n",
    "test_local_file = os.path.join(local_temp, s3_uri_test.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.s3.S3Downloader.download(s3_uri_test_classes, test_local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv\n",
    "\n",
    "from datasets.chemprot_dataset import ChemprotDataset\n",
    "from datasets.chemprot_ner_label_mapper import ChemprotNerLabelMapper\n",
    "\n",
    "def chunk(l, size=5):\n",
    "    for i in range(0, len(l),size):\n",
    "        yield l[i:i+size]\n",
    "        \n",
    "def predict(test_local_file, output_file):\n",
    "    \n",
    "    # Load file\n",
    "    with open(test_local_file, \"r\") as f:\n",
    "        docs = []\n",
    "        ids =[]\n",
    "        reader = csv.reader(f, delimiter='\\t', quotechar=None)\n",
    "        for l in reader:\n",
    "            (id, text) = l[0], l[1] + l[2]\n",
    "            docs.append(text)\n",
    "            ids.append(id)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    label_mapper = ChemprotNerLabelMapper()\n",
    "\n",
    "    id_chucks = list(chunk(ids))\n",
    "    result = []\n",
    "    for (i, data) in enumerate(chunk(docs)):\n",
    "\n",
    "        data_bytes=(\"\\n\".join(data)).encode(\"utf-8\")\n",
    "        response  = predictor.predict(data_bytes,  \n",
    "                                        initial_args={ \"Accept\":\"text/json\", \"ContentType\" : \"text/csv\" }\n",
    "                                           )\n",
    "\n",
    "        assert len(response) == len(data), \"Data size {} doesnt match result size {}\".format(len(r), len(d))\n",
    "\n",
    "\n",
    "\n",
    "        for ri, r in enumerate(response):\n",
    "            doc_id = id_chucks[i][ri]\n",
    "\n",
    "            result.append({\"docid\":doc_id, \"text\": data[ri], \"entities_detected\": r })\n",
    "        \n",
    "    \n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump( result, f)\n",
    "        \n",
    "\n",
    "\n",
    "results_json_file=os.path.join(local_temp, \"result.json\")\n",
    "predict(test_local_file, results_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-23 17:18:37,879 - sagemaker - INFO - Deleting endpoint configuration with name: pytorch-inference-2022-07-24-00-13-41-043\n",
      "2022-07-23 17:18:38,049 - sagemaker - INFO - Deleting endpoint with name: pytorch-inference-2022-07-24-00-13-41-043\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}